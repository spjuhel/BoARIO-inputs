from snakemake.utils import Paramspace
import pathlib

configfile: "workflow/config.json"
include: "./rules/init.smk"
include: "./rules/common.smk"
include: "./rules/mrio_generation.smk"

ruleorder: generate_mrio_full_from_zip > mrio_sector_aggreg
ruleorder: mrio_sector_aggreg > mrio_one_region_RoW_aggreg

wildcard_constraints:
    run_type="raw|int",
    shock_type="rebuilding|recover"

check_config(config)

#TODO : integrate interpolation script

localrules:generate_csv_from_all_xp, init_all_sim_parquet, xp_parquet, indicators

rule generate_drias:
    """
    Generate files for R treatment
    """
    input:
        donefile=expand("{outputdir}/{{expdir}}/.aggregation_done", outputdir=config["OUTPUT_DIR"]),
        outdir=directory(expand("{outputdir}/{{expdir}}/aggreg", outputdir=config["OUTPUT_DIR"]))
    output:
        expand("{outputdir}/{{expdir}}/aggreg/{loss_type}_drias_carre_essai{nomean}.parquet", outputdir=config["OUTPUT_DIR"], loss_type=["prodloss","finalloss"], nomean=["","_nomean"])
    conda:
        "env/boario-use.yml"
    shell:
        """
        python {config[INPUTS_GENERATION_SCRIPTS_DIR]}/parquet_for_R.py --input-dir {input.outdir}
        """

rule aggreg_all_xp:
    """
    Aggregate and interpolate all xp results with flood db.
    """
    input:
        expand("{outputdir}/{expdir}/.aggregation_done", outputdir=config["OUTPUT_DIR"],expdir=config["EXPS"])

rule aggreg_xp:
    """
    Aggregate and interpolate xp results with flood db.
    """
    input:
        generalcsv = expand("{outputdir}/{{expdir}}/general.csv", outputdir=config["OUTPUT_DIR"]),
        prodlosscsv = expand("{outputdir}/{{expdir}}/prodloss.csv", outputdir=config["OUTPUT_DIR"]),
        finallosscsv = expand("{outputdir}/{{expdir}}/finalloss.csv", outputdir=config["OUTPUT_DIR"]),
        floodbase = find_floodbase,
        rep_events = find_repevents
    params:
        period = find_periodname
    resources:
        vmem_mb=64000,
        mem_mb=64000,
        disk_mb=1000,
        partition="zen16",
        time=120
    threads:
        4
    output:
        donefile=expand("{outputdir}/{{expdir}}/.aggregation_done", outputdir=config["OUTPUT_DIR"]),
        outdir=directory(expand("{outputdir}/{{expdir}}/aggreg", outputdir=config["OUTPUT_DIR"]))
    shell:
        """
        /home/sjuhel/mambaforge/envs/versa/bin/python {config[INPUTS_GENERATION_SCRIPTS_DIR]}/prep_dfs.py --input-general {input.generalcsv} --input-loss {input.prodlosscsv} --loss-type prod --flood-base {input.floodbase} --representative {input.rep_events} --period-name {params.period} --output {output.outdir};
        /home/sjuhel/mambaforge/envs/versa/bin/python {config[INPUTS_GENERATION_SCRIPTS_DIR]}/prep_dfs.py --input-general {input.generalcsv} --input-loss {input.finallosscsv} --loss-type final --flood-base {input.floodbase} --representative {input.rep_events} --period-name {params.period} --output {output.outdir}
        touch {output.donefile}
        """

rule generate_csv_from_all_xp:
    """
    Generate csv file aggregating results from all experiences stated in config.json
    """
    input:
        all_sim = "{}/all_simulations.parquet".format(config["BUILDED_DATA_DIR"]),
        csv = csv_from_all_xp(expand("{exp_dir}/{exp_jsons}.json",exp_dir=config["EXPS_JSONS"],exp_jsons=config["EXPS"]))

rule init_all_sim_parquet:
    input:
        config = expand("{exp_dir}/config.json",exp_dir=config["EXPS_JSONS"]),
        xps = expand("{exp_dir}/{exp_jsons}.json",exp_dir=config["EXPS_JSONS"],exp_jsons=config["EXPS"])
    output:
        all_sim = "{}/all_simulations.parquet".format(config["BUILDED_DATA_DIR"])
    run:
        meta_df = pd.DataFrame()
        for xp in input.xps:
            xp_df = sim_df_from_xp(xp)
            meta_df = pd.concat([meta_df, xp_df],axis=0)
        meta_df.to_parquet(output.all_sim)

rule generate_csv_from_xp:
    """
    Generate csv files for a specific experience.
    """
    input:
        xp_json = expand("{exp_dir}/{{expdir}}.json",exp_dir=config["EXPS_JSONS"]),
        xp_done = expand("{outputdir}/{{expdir}}/.experience_done", outputdir=config["OUTPUT_DIR"])
    output:
        expand("{outputdir}/{{expdir}}/{files}.csv", outputdir=config["OUTPUT_DIR"], files=["general", "prodloss","finalloss"])
    conda:
        "env/boario-use.yml"
    params:
        dirs = expand("{outputdir}/{{expdir}}/", outputdir=config["OUTPUT_DIR"])
    resources:
        vmem_mb=500,
        mem_mb=500,
        disk_mb=500
    shell:
        """
        python {config[INPUTS_GENERATION_SCRIPTS_DIR]}/csv_from_indicators.py {params.dirs} -o {params.dirs}
        """

rule xp_parquet:
    """
    Requires (invoke building rules for) all indicators files for an experience (specified by {expdir})
    """
    input:
        runs_from_expdir
    output:
        touch(expand("{outputdir}/{{expdir}}/.experience_done", outputdir=config["OUTPUT_DIR"]))


rule indicators:
    """
    Build symlinks to indicators and parquet results files for a specific simulation.
    """
    input:
        unpack(input_for_indicators_symlinks)
    output:
        a = directory(expand("{out}/{{xp_folder}}/{{mrio_used}}/{{params_group}}/{{region}}/{{ev_class}}/indicators", out=config["OUTPUT_DIR"])),
        b = touch(expand("{out}/{{xp_folder}}/{{mrio_used}}/{{params_group}}/{{region}}/{{ev_class}}/{{xp_folder}}~{{mrio_used}}~{{params_group}}~{{region}}~{{ev_class}}.name",
                            out=config["OUTPUT_DIR"]))
    run:
        dir_dest = Path(output.a[0]).parent
        for f in input:
            origin = Path(f)
            dest = dir_dest/origin.name
            if not dest.exists():
                dest.symlink_to(origin)


rule indicators_generic:
    """
    Build indicators and parquet results files for a specific simulation.
    """
    input:
        expand("{inp}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/records",
               inp=config["OUTPUT_DIR"])
    output:
        inds = directory(expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/indicators",
               out=config["OUTPUT_DIR"])),
        parquets =directory(expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/parquets",
                                   out=config["OUTPUT_DIR"]))
    params:
        records=directory(expand("{inp}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/records",
               inp=config["OUTPUT_DIR"]))
    #group: "sim_run"
    conda:
        "env/boario-use.yml"
    resources:
        vmem_mb=8000, #indicators_get_vmem_mb,
        mem_mb=8000, #indicators_get_mem_mb,
        disk_mb=500 #indicators_get_disk_mb
    shell:
        """
        cd {config[ARIO_DIR]};
        python ./scripts/indicator_from_folder.py '{config[OUTPUT_DIR]}/{wildcards.mrio_used}/{wildcards.params_group}/{wildcards.region}/{wildcards.dmg_as_pct}_{wildcards.duration}';
        """
rule run_generic:
    """
    Generic run
    """
    input:
        unpack(run_inputs2)
    output:
        record_files =  temp(directory(expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/records",
               out=config["OUTPUT_DIR"]))),
        json_files = directory(expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/jsons",
                            out=config["OUTPUT_DIR"]))
    #group: "sim_run"
    params:
        output_dir = config["OUTPUT_DIR"]
    threads:
        4
    log:
        expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/simulation.log", out=config["OUTPUT_DIR"])
    resources:
        vmem_mb=4000,
        mem_mb=4000,
        disk_mb=500
    wildcard_constraints:
        region="[A-Z]{2}"
    conda:
        "env/boario-use.yml"
    shell:
        """
        cd {config[ARIO_DIR]};
        nice -n 10 python ./scripts/mono_run_2.py {wildcards.mrio_used} {wildcards.region} {wildcards.dmg_as_pct} {wildcards.duration} {wildcards.params_group} {input.mrio} {params.output_dir}
        """

rule run_Full:
    """
    Run a "Full" simulation (no mrio region agregation or disagregation).

    run_inputs(wildcards) is in common.smk
    """
    input:
        unpack(run_inputs),
        event_template = lambda wildcards : get_event_template(wildcards.mrio_used,wildcards.shock_type),
        mrio_params = lambda wildcards : get_mrio_params(wildcards.mrio_used,wildcards.xp_folder)
    output:
        record_files =  temp(directory(expand("{out}/{{xp_folder}}/{{mrio_used}}/{{region}}_type_{{shock_type}}_qdmg_{{run_type}}_{{flood}}_Psi_{{psi}}_inv_tau_{{inv}}_inv_time_{{inv_t}}/records", out=config["OUTPUT_DIR"]))),
        json_files = directory(expand("{out}/{{xp_folder}}/{{mrio_used}}/{{region}}_type_{{shock_type}}_qdmg_{{run_type}}_{{flood}}_Psi_{{psi}}_inv_tau_{{inv}}_inv_time_{{inv_t}}/jsons", out=config["OUTPUT_DIR"]))
    #group: "sim_run"
    params:
        output_dir = config["OUTPUT_DIR"]
    threads:
        4
    log:
        expand("{out}/{{xp_folder}}/{{mrio_used}}/{{region}}_type_{{shock_type}}_qdmg_{{run_type}}_{{flood}}_Psi_{{psi}}_inv_tau_{{inv}}_inv_time_{{inv_t}}/simulation.log",
               out=config["OUTPUT_DIR"])
    resources:
        vmem_mb=run_Full_get_vmem_mb,
        mem_mb=run_Full_get_mem_mb,
        disk_mb=run_Full_get_disk_mb
    wildcard_constraints:
        region="[A-Z]{2}"
    conda:
        "env/boario-use.yml"
    shell:
        """
        cd {config[ARIO_DIR]};
        nice -n 10 python ./scripts/mono_run.py {wildcards.region} {input.params_template} {wildcards.psi} {wildcards.inv} {wildcards.shock_type} {wildcards.run_type} {wildcards.flood} {input.mrio} {params.output_dir}/{wildcards.xp_folder}/{wildcards.mrio_used} {input.flood_gdp} {input.event_template} {input.mrio_params} {wildcards.inv_t}
        """

rule run_subregions_mrio:
    """
    Run simulation where one region has been cut in subregions.
    (Probably deprecated)
    """
    input:
        unpack(run_inputs)
    output:
        expand("{out}/{{xp_folder}}/{{mrio_used}}/{{region}}_type_Subregions_qdmg_{{run_type}}_{{flood}}_Psi_{{psi}}_inv_tau_{{inv}}_inv_time_{{inv_t}}/{files}",
               out=config["OUTPUT_DIR"],
               files=run_output_files)
    params:
        output_dir = config["OUTPUT_DIR"],
        event_template = lambda wildcards : get_event_template(wildcards.mrio_used,wildcards.xp_folder),
        mrio_params = lambda wildcards : get_mrio_params(wildcards.mrio_used,wildcards.xp_folder)
    threads:
        8
    resources:
        vmem_mb=3000,
        mem_mb=2000,
        disk_mb=500
    wildcard_constraints:
        region="(?:[A-Z]{2}-[A-Z]{2}\d+)|([A-Z]{2}-all)",
        mrio_used="exiobase3_(?:Full|\d+_sectors)_(?:FullWorld|[A-Z]{2}-RoW)_[A-Z]{2}_sliced_in_\d+"
    conda:
        "env/boario-use.yml"
    params:
        output_dir = config["OUTPUT_DIR"],
    shell:
        """
        cd {config[ARIO_DIR]};
        nice -n 10 python ./scripts/mono_run.py {wildcards.region} {input.params_template} {wildcards.psi} {wildcards.inv} Full {wildcards.run_type} {wildcards.flood} {input.mrio} {params.output_dir}/{wildcards.xp_folder}/{wildcards.mrio_used} {input.flood_gdp} {params.event_template} {params.mrio_params} {wildcards.inv_t}
        """

rule run_RoW:
    """
    Run 'Rest of the World' simulation, where all regions expect the chosen one are agregated into a RoW region. (For testing/sensitivity purpose, might be broken at the moment)
    """
    input:
        unpack(run_RoW_inputs)
    output:
        expand("{out}/{{xp_folder}}/{{mrio_used}}/{{region}}_type_RoW_qdmg_{{run_type}}_{{flood}}_Psi_{{psi}}_inv_tau_{{inv}}_inv_time_{{inv_t}}/{files}",
               out=config["OUTPUT_DIR"],
               files=run_output_files)
    conda:
        "env/boario-use.yml"
    wildcard_constraints:
        run_type="raw|int"
    resources:
        mem_mb=4000,
        disk_mb=50,
        vmem_mb=4000
    params:
        ario_dir = config["ARIO_DIR"],
        output_dir = config["OUTPUT_DIR"],
    shell:
        """
        cd {params.ario_dir};
        nice -n 10 python ./scripts/mono_run.py {wildcards.region} {input.params_template} {wildcards.psi} {wildcards.inv} RoW {wildcards.run_type} {wildcards.flood} {input.mrio} {params.output_dir}/{wildcards.xp_folder}/{wildcards.mrio_used} {input.flood_gdp} {input.event_template} {input.mrio_params} {wildcards.inv_t}
        """
