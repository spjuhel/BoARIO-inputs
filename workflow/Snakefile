from snakemake.utils import Paramspace
import pathlib

configfile: "workflow/config.json"
include: "./rules/init.smk"
include: "./rules/common.smk"
include: "./rules/mrio_generation.smk"
include: "./rules/flood_generation.smk"
include: "./rules/resources.smk"

ruleorder: generate_exiobase3_full_from_zip > mrio_sector_aggreg
ruleorder: mrio_sector_aggreg > mrio_one_region_RoW_aggreg

wildcard_constraints:
    run_type="raw|int",
    shock_type="rebuilding|recover"


localrules:generate_csv_from_all_xp, init_all_sim_parquet, xp_parquet, indicators

rule generate_all_drias:
    """
    Generate all the files for R treatment from all xp
    """
    input:
        expand("{outputdir}/{expdir}/aggreg/{loss_type}_drias_carre_essai{nomean}.parquet" ,expdir=config["EXPS"], outputdir=config["OUTPUT_DIR"], loss_type=["prodloss","finalloss"], nomean=["","_nomean"])

rule generate_drias:
    """
    Generate files for R treatment
    """
    input:
        donefile=expand("{outputdir}/{{expdir}}/.aggregation_done", outputdir=config["OUTPUT_DIR"])
    params:
        outdir=expand("{outputdir}/{{expdir}}/aggreg", outputdir=config["OUTPUT_DIR"])
    output:
        expand("{outputdir}/{{expdir}}/aggreg/{loss_type}_drias_carre_essai{nomean}.parquet", outputdir=config["OUTPUT_DIR"], loss_type=["prodloss","finalloss"], nomean=["","_nomean"])
    conda:
        "boario-use"
    resources:
        mem_mb=32000
    threads:
        4
    shell:
        """
        python {config[INPUTS_GENERATION_SCRIPTS_DIR]}/parquet_for_R.py --input-dir {params.outdir}
        """

rule aggreg_all_xp:
    """
    Aggregate and interpolate all xp results with flood db.
    """
    input:
        expand("{outputdir}/{expdir}/.aggregation_done", outputdir=config["OUTPUT_DIR"],expdir=config["EXPS"])

rule aggreg_xp:
    """
    Aggregate and interpolate xp results with flood db.
    """
    input:
        generalcsv = expand("{outputdir}/{{expdir}}/general.csv", outputdir=config["OUTPUT_DIR"]),
        prodlosscsv = expand("{outputdir}/{{expdir}}/prodloss.csv", outputdir=config["OUTPUT_DIR"]),
        finallosscsv = expand("{outputdir}/{{expdir}}/finalloss.csv", outputdir=config["OUTPUT_DIR"]),
        floodbase = find_floodbase,
        rep_events = find_repevents
    params:
        outdir=directory(expand("{outputdir}/{{expdir}}/aggreg", outputdir=config["OUTPUT_DIR"])),
        period = find_periodname
    resources:
        vmem_mb=64000,
        mem_mb=64000,
        disk_mb=1000,
        partition="zen16",
        time=120
    threads:
        4
    output:
        donefile=expand("{outputdir}/{{expdir}}/.aggregation_done", outputdir=config["OUTPUT_DIR"])
    shell:
        """
        /home/sjuhel/mambaforge/envs/versa/bin/python {config[INPUTS_GENERATION_SCRIPTS_DIR]}/prep_dfs.py --input-general {input.generalcsv} --input-loss {input.prodlosscsv} --loss-type prod --flood-base {input.floodbase} --representative {input.rep_events} --period-name {params.period} --output {params.outdir};
        /home/sjuhel/mambaforge/envs/versa/bin/python {config[INPUTS_GENERATION_SCRIPTS_DIR]}/prep_dfs.py --input-general {input.generalcsv} --input-loss {input.finallosscsv} --loss-type final --flood-base {input.floodbase} --representative {input.rep_events} --period-name {params.period} --output {params.outdir}
        touch {output.donefile}
        """

rule generate_csv_from_all_xp:
    """
    Generate csv file aggregating results from all experiences stated in config.json
    """
    input:
        all_sim = "{}/all_simulations.parquet".format(config["BUILDED_DATA_DIR"]),
        csv = csv_from_all_xp(expand("{exp_dir}/{exp_jsons}.json",exp_dir=config["EXPS_JSONS"],exp_jsons=config["EXPS"]))

rule init_all_sim_parquet:
    input:
        config = expand("{exp_dir}/config.json",exp_dir=config["EXPS_JSONS"]),
        xps = expand("{exp_dir}/{exp_jsons}.json",exp_dir=config["EXPS_JSONS"],exp_jsons=config["EXPS"])
    output:
        all_sim = "{}/all_simulations.parquet".format(config["BUILDED_DATA_DIR"])
    run:
        meta_df = pd.DataFrame()
        for xp in input.xps:
            xp_df = init_sim_df_from_xp(xp)
            meta_df = pd.concat([meta_df, xp_df],axis=0)
        meta_df.to_parquet(output.all_sim)

rule generate_csv_from_xp:
    """
    Generate csv files for a specific experience.
    """
    input:
        xp_json = expand("{exp_dir}/{{expdir}}.json",exp_dir=config["EXPS_JSONS"]),
        xp_done = expand("{outputdir}/{{expdir}}/.experience_done", outputdir=config["OUTPUT_DIR"])
    output:
        expand("{outputdir}/{{expdir}}/{files}.csv", outputdir=config["OUTPUT_DIR"], files=["general", "prodloss","finalloss"])
    conda:
        "boario-use"
    params:
        dirs = expand("{outputdir}/{{expdir}}/", outputdir=config["OUTPUT_DIR"])
    resources:
        vmem_mb=500,
        mem_mb=500,
        disk_mb=500
    shell:
        """
        python {config[INPUTS_GENERATION_SCRIPTS_DIR]}/csv_from_indicators.py {params.dirs} -o {params.dirs}
        """

rule xp_parquet:
    """
    Requires (invoke building rules for) all indicators files for an experience (specified by {expdir})
    """
    input:
        runs_from_expdir
    output:
        touch(expand("{outputdir}/{{expdir}}/.experience_done", outputdir=config["OUTPUT_DIR"]))


rule indicators:
    """
    Build symlinks to indicators and parquet results files for a specific simulation.
    """
    input:
        unpack(input_for_indicators_symlinks)
    output:
        b = touch(expand("{out}/{{xp_folder}}/{{mrio_used}}/{{params_group}}/{{region}}/{{ev_class}}/{{xp_folder}}~{{mrio_used}}~{{params_group}}~{{region}}~{{ev_class}}.name",
                            out=config["OUTPUT_DIR"]))
    params:
        a = directory(expand("{out}/{{xp_folder}}/{{mrio_used}}/{{params_group}}/{{region}}/{{ev_class}}/indicators", out=config["OUTPUT_DIR"]))
    run:
        dir_dest = Path(params.a[0]).parent
        for f in input:
            origin = Path(f)
            dest = dir_dest/origin.name
            if not dest.exists():
                dest.symlink_to(origin)


rule indicators_generic:
    """
    Build indicators and parquet results files for a specific simulation.
    """
    input:
        expand("{inp}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/records",
               inp=config["OUTPUT_DIR"])
    output:
        inds = directory(expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/indicators",
               out=config["OUTPUT_DIR"])),
        parquets =directory(expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/parquets",
                                   out=config["OUTPUT_DIR"]))
    params:
        records=directory(expand("{inp}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/records",
               inp=config["OUTPUT_DIR"]))
    #group: "sim_run"
    conda:
        "boario-use"
    resources:
        vmem_mb=8000, #indicators_get_vmem_mb,
        mem_mb=8000, #indicators_get_mem_mb,
        disk_mb=500 #indicators_get_disk_mb
    shell:
        """
        cd {config[ARIO_DIR]};
        python ./scripts/indicator_from_folder.py '{config[OUTPUT_DIR]}/{wildcards.mrio_used}/{wildcards.params_group}/{wildcards.region}/{wildcards.dmg_as_pct}_{wildcards.duration}';
        """
rule run_generic:
    """
    Generic run
    """
    input:
        unpack(run_inputs)
    output:
        record_files =  temp(directory(expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/records",
               out=config["OUTPUT_DIR"]))),
        json_files = directory(expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/jsons",
                            out=config["OUTPUT_DIR"]))
    #group: "sim_run"
    params:
        output_dir = config["OUTPUT_DIR"]
    threads:
        4
    log:
        expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/simulation.log", out=config["OUTPUT_DIR"])
    resources:
        vmem_mb=4000,
        mem_mb=4000,
        disk_mb=500
    wildcard_constraints:
        region="[A-Z]{2}"
    conda:
        "boario-use"
    shell:
        """
        python {config[INPUTS_GENERATION_SCRIPTS_DIR]}/generic_run.py {wildcards.mrio_used} {wildcards.region} {wildcards.dmg_as_pct} {wildcards.duration} {wildcards.params_group} {input.mrio} {params.output_dir}
        """
