from snakemake.utils import Paramspace
import pathlib

configfile: "workflow/config.json"
include: "./rules/init.smk"
include: "./rules/common.smk"
include: "./rules/mrio_generation.smk"

ruleorder: generate_mrio_full_from_zip > mrio_sector_aggreg
ruleorder: mrio_sector_aggreg > mrio_one_region_RoW_aggreg

wildcard_constraints:
    run_type="raw|int",
    shock_type="rebuilding|recover"

check_config(config)

#TODO : integrate interpolation script

rule generate_csv_from_all_xp:
    """
    Generate csv file aggregating results from all experiences stated in config.json
    """
    input:
        all_sim = "{}/all_simulations.parquet".format(config["BUILDED_DATA_DIR"]),
        csv = csv_from_all_xp(expand("{exp_dir}/{exp_jsons}.json",exp_dir=config["EXPS_JSONS"],exp_jsons=config["EXPS"]))

rule init_all_sim_parquet:
    input:
        config = expand("{exp_dir}/config.json",exp_dir=config["EXPS_JSONS"]),
        xps = expand("{exp_dir}/{exp_jsons}.json",exp_dir=config["EXPS_JSONS"],exp_jsons=config["EXPS"])
    output:
        all_sim = "{}/all_simulations.parquet".format(config["BUILDED_DATA_DIR"])
    run:
        meta_df = pd.DataFrame()
        for xp in input.xps:
            xp_df = sim_df_from_xp(xp)
            meta_df = pd.concat([meta_df, xp_df],axis=0)
        meta_df.to_parquet(output.all_sim)

rule generate_csv_from_xp:
    """
    Generate csv files for a specific experience.
    """
    input:
        xp_json = expand("{exp_dir}/{{expdir}}.json",exp_dir=config["EXPS_JSONS"]),
        xp_done = expand("{outputdir}/{{expdir}}/.experience_done", outputdir=config["OUTPUT_DIR"])
    output:
        expand("{outputdir}/{{expdir}}/{{run_type}}_{files}.csv", outputdir=config["OUTPUT_DIR"], files=["general", "prodloss","fdloss"]),
        touch(expand("{outputdir}/{{expdir}}/.{{run_type}}_experience_csv_done", outputdir=config["OUTPUT_DIR"]))
    conda:
        "env/boario-use.yml"
    params:
        dirs = expand("{outputdir}/{{expdir}}/", outputdir=config["OUTPUT_DIR"])
    resources:
        vmem_mb=500,
        mem_mb=500,
        disk_mb=500
    shell:
        """
        nice -n 10 python {config[INPUTS_GENERATION_SCRIPTS_DIR]}/csv_from_indicators.py {params.dirs} {wildcards.run_type} -o {params.dirs}
        """

rule xp_parquet:
    """
    Requires (invoke building rules for) all indicators files for an experience (specified by {expdir})
    """
    input:
        runs_from_expdir
    output:
        touch(expand("{outputdir}/{{expdir}}/.experience_done", outputdir=config["OUTPUT_DIR"]))


rule indicators:
    """
    Build symlinks to indicators and parquet results files for a specific simulation.
    """
    input:
        input_for_indicators_symlinks
    output:
        a = expand("{out}/{{xp_folder}}/{{mrio_used}}/{{params_group}}/{{region}}/{{ev_class}}/{files}", out=config["OUTPUT_DIR"], files=["indicators.json",
                   "treated_df_limiting.parquet",
                   "treated_df_loss.parquet",
                   "prod_df.parquet",
                   "io_demand_df.parquet",
                   "final_demand_df.parquet",
                   "prod_chg.json",
                   "fd_loss.json"]),
        b = touch(expand("{out}/{{xp_folder}}/{{mrio_used}}/{{params_group}}/{{region}}/{{ev_class}}/{{xp_folder}}~{{mrio_used}}~{{params_group}}~{{region}}~{{ev_class}}.name",
                            out=config["OUTPUT_DIR"]))
    run:
        dir_dest = Path(output.a[0]).parent
        for f in input:
            origin = Path(f)
            dest = dir_dest/origin.name
            if not dest.exists():
                dest.symlink_to(origin)


rule indicators_generic:
    """
    Build indicators and parquet results files for a specific simulation.
    """
    input:
        expand("{inp}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/{files}",
               inp=config["OUTPUT_DIR"],
               files=run_output_files)
    output:
        expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/{files}",
               out=config["OUTPUT_DIR"],
               files=[
                   "indicators.json",
                   "treated_df_limiting.parquet",
                   "treated_df_loss.parquet",
                   "prod_df.parquet",
                   "io_demand_df.parquet",
                   "final_demand_df.parquet",
                   "prod_chg.json",
                   "fd_loss.json"])
    params:
        records=expand("{inp}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/{files}",
               inp=config["OUTPUT_DIR"],
               files=record_files)
    #group: "sim_run"
    conda:
        "env/boario-use.yml"
    resources:
        vmem_mb=8000, #indicators_get_vmem_mb,
        mem_mb=8000, #indicators_get_mem_mb,
        disk_mb=500 #indicators_get_disk_mb
    shell:
        """
        cd {config[ARIO_DIR]};
        nice -n 10 python ./scripts/indicator_from_folder.py '{config[OUTPUT_DIR]}/{wildcards.mrio_used}/{wildcards.params_group}/{wildcards.region}/{wildcards.dmg_as_pct}_{wildcards.duration}';
        """
rule run_generic:
    """
    Generic run
    """
    input:
        unpack(run_inputs2)
    output:
        record_files =  temp(expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/{files}",
               out=config["OUTPUT_DIR"],
               files=record_files)),
        json_files = expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/{files}",
                            out=config["OUTPUT_DIR"],
                            files=run_json_files)
    #group: "sim_run"
    params:
        output_dir = config["OUTPUT_DIR"]
    threads:
        4
    log:
        expand("{out}/{{mrio_used}}/{{params_group}}/{{region}}/{{dmg_as_pct}}_{{duration}}/simulation.log", out=config["OUTPUT_DIR"])
    resources:
        vmem_mb=4000,
        mem_mb=4000,
        disk_mb=500
    wildcard_constraints:
        region="[A-Z]{2}"
    conda:
        "env/boario-use.yml"
    shell:
        """
        cd {config[ARIO_DIR]};
        nice -n 10 python ./scripts/mono_run_2.py {wildcards.mrio_used} {wildcards.region} {wildcards.dmg_as_pct} {wildcards.duration} {wildcards.params_group} {input.mrio} {params.output_dir}
        """

rule run_Full:
    """
    Run a "Full" simulation (no mrio region agregation or disagregation).

    run_inputs(wildcards) is in common.smk
    """
    input:
        unpack(run_inputs),
        event_template = lambda wildcards : get_event_template(wildcards.mrio_used,wildcards.shock_type),
        mrio_params = lambda wildcards : get_mrio_params(wildcards.mrio_used,wildcards.xp_folder)
    output:
        record_files =  temp(expand("{out}/{{xp_folder}}/{{mrio_used}}/{{region}}_type_{{shock_type}}_qdmg_{{run_type}}_{{flood}}_Psi_{{psi}}_inv_tau_{{inv}}_inv_time_{{inv_t}}/{files}",
               out=config["OUTPUT_DIR"],
               files=record_files)),
        json_files = expand("{out}/{{xp_folder}}/{{mrio_used}}/{{region}}_type_{{shock_type}}_qdmg_{{run_type}}_{{flood}}_Psi_{{psi}}_inv_tau_{{inv}}_inv_time_{{inv_t}}/{files}",
                            out=config["OUTPUT_DIR"],
                            files=run_json_files)
    #group: "sim_run"
    params:
        output_dir = config["OUTPUT_DIR"]
    threads:
        4
    log:
        expand("{out}/{{xp_folder}}/{{mrio_used}}/{{region}}_type_{{shock_type}}_qdmg_{{run_type}}_{{flood}}_Psi_{{psi}}_inv_tau_{{inv}}_inv_time_{{inv_t}}/simulation.log",
               out=config["OUTPUT_DIR"])
    resources:
        vmem_mb=run_Full_get_vmem_mb,
        mem_mb=run_Full_get_mem_mb,
        disk_mb=run_Full_get_disk_mb
    wildcard_constraints:
        region="[A-Z]{2}"
    conda:
        "env/boario-use.yml"
    shell:
        """
        cd {config[ARIO_DIR]};
        nice -n 10 python ./scripts/mono_run.py {wildcards.region} {input.params_template} {wildcards.psi} {wildcards.inv} {wildcards.shock_type} {wildcards.run_type} {wildcards.flood} {input.mrio} {params.output_dir}/{wildcards.xp_folder}/{wildcards.mrio_used} {input.flood_gdp} {input.event_template} {input.mrio_params} {wildcards.inv_t}
        """

rule run_subregions_mrio:
    """
    Run simulation where one region has been cut in subregions.
    (Probably deprecated)
    """
    input:
        unpack(run_inputs)
    output:
        expand("{out}/{{xp_folder}}/{{mrio_used}}/{{region}}_type_Subregions_qdmg_{{run_type}}_{{flood}}_Psi_{{psi}}_inv_tau_{{inv}}_inv_time_{{inv_t}}/{files}",
               out=config["OUTPUT_DIR"],
               files=run_output_files)
    params:
        output_dir = config["OUTPUT_DIR"],
        event_template = lambda wildcards : get_event_template(wildcards.mrio_used,wildcards.xp_folder),
        mrio_params = lambda wildcards : get_mrio_params(wildcards.mrio_used,wildcards.xp_folder)
    threads:
        8
    resources:
        vmem_mb=3000,
        mem_mb=2000,
        disk_mb=500
    wildcard_constraints:
        region="(?:[A-Z]{2}-[A-Z]{2}\d+)|([A-Z]{2}-all)",
        mrio_used="exiobase3_(?:Full|\d+_sectors)_(?:FullWorld|[A-Z]{2}-RoW)_[A-Z]{2}_sliced_in_\d+"
    conda:
        "env/boario-use.yml"
    params:
        output_dir = config["OUTPUT_DIR"],
    shell:
        """
        cd {config[ARIO_DIR]};
        nice -n 10 python ./scripts/mono_run.py {wildcards.region} {input.params_template} {wildcards.psi} {wildcards.inv} Full {wildcards.run_type} {wildcards.flood} {input.mrio} {params.output_dir}/{wildcards.xp_folder}/{wildcards.mrio_used} {input.flood_gdp} {params.event_template} {params.mrio_params} {wildcards.inv_t}
        """

rule run_RoW:
    """
    Run 'Rest of the World' simulation, where all regions expect the chosen one are agregated into a RoW region. (For testing/sensitivity purpose, might be broken at the moment)
    """
    input:
        unpack(run_RoW_inputs)
    output:
        expand("{out}/{{xp_folder}}/{{mrio_used}}/{{region}}_type_RoW_qdmg_{{run_type}}_{{flood}}_Psi_{{psi}}_inv_tau_{{inv}}_inv_time_{{inv_t}}/{files}",
               out=config["OUTPUT_DIR"],
               files=run_output_files)
    conda:
        "env/boario-use.yml"
    wildcard_constraints:
        run_type="raw|int"
    resources:
        mem_mb=4000,
        disk_mb=50,
        vmem_mb=4000
    params:
        ario_dir = config["ARIO_DIR"],
        output_dir = config["OUTPUT_DIR"],
    shell:
        """
        cd {params.ario_dir};
        nice -n 10 python ./scripts/mono_run.py {wildcards.region} {input.params_template} {wildcards.psi} {wildcards.inv} RoW {wildcards.run_type} {wildcards.flood} {input.mrio} {params.output_dir}/{wildcards.xp_folder}/{wildcards.mrio_used} {input.flood_gdp} {input.event_template} {input.mrio_params} {wildcards.inv_t}
        """
